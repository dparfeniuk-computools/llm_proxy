# Use a slim base image with Python
FROM python:3.12-slim

# Set up environment variables
ENV PYTHONUNBUFFERED=1

# Install necessary system dependencies
RUN apt-get update \
    && apt-get install -y build-essential git \
    libcurl4-openssl-dev libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Clone and build llama.cpp
RUN git clone https://github.com/ggerganov/llama.cpp /llama.cpp \
    && cd /llama.cpp \
    && make

WORKDIR /app/

# Llama Python dependencies
COPY requirements/llama.txt /app/requirements/llama.txt

RUN pip install -r requirements/llama.txt

# Expose the default llama-server port
EXPOSE 8080

# Download the model from Hugging Face
RUN huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False

# Run llama-server when the container starts
CMD ["/bin/sh", "-c", "/llama.cpp/llama-server -m $LLAMA_MODEL_PATH --host 0.0.0.0"]
